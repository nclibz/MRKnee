{"cells":[{"cell_type":"code","execution_count":null,"source":["import os\n","\n","import albumentations as A\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from src.model import MRKnee\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!git clone https://github.com/nclibz/MRKnee/\n","import os\n","os.chdir('/content/MRKnee/')\n","!git checkout lstm"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\n","# INSTANTIATE MODEL\n","\n","backbone = \"efficientnet_b1\"\n","plane = \"axial\"\n","ckpt = \"src/models/acl_axial.ckpt\"\n","img_sz = 240\n","device = \"cpu\"\n","\n","\n","model = MRKnee.load_from_checkpoint(ckpt, planes=[plane], backbone=backbone)\n","model.eval()\n","model.to(device=device)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# INPUT\n","paths = []\n","for root, dirs, files in os.walk(os.path.abspath(\"data/valid\")):\n","    for file in files:\n","        if plane in root:\n","            paths.append(os.path.join(root, file))\n","\n","\n","path = paths[0]\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\n","\n","def do_aug(imgs, transf):\n","    img_dict = {}\n","    target_dict = {}\n","    for i in range(imgs.shape[0]):\n","        if i == 0:\n","            img_dict[\"image\"] = imgs[i, :, :]\n","        else:\n","            img_name = \"image\" + f\"{i}\"\n","            img_dict[img_name] = imgs[i, :, :]\n","            target_dict[img_name] = \"image\"\n","    transf = A.Compose(transf)\n","    transf.add_targets(target_dict)\n","    out = transf(**img_dict)\n","    out = list(out.values())\n","    return out  # returns list of np arrays\n","\n","\n","imgs = np.load(path)\n","imgs = do_aug(imgs, transf=[A.CenterCrop(img_sz, img_sz)])\n","imgs = torch.as_tensor(imgs, dtype=torch.float32)\n","imgs = (imgs - imgs.min()) / (imgs.max() - imgs.min()) * 255\n","if plane == \"axial\":\n","    MEAN, SD = 66.4869, 60.8146\n","elif plane == \"sagittal\":\n","    MEAN, SD = 60.0440, 48.3106\n","elif plane == \"coronal\":\n","    MEAN, SD = 61.9277, 64.2818\n","imgs = (imgs - MEAN) / SD\n","imgs = imgs.unsqueeze(1)  # create channel dim\n","imgs = imgs.to(device=device)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["def model_wrapper(input_batch):  # nx1x240x240\n","    print(input_batch.shape)\n","    input_batch = input_batch.unsqueeze(0)  # create batch dim\n","    print(input_batch.shape)\n","    res = model(input_batch)\n","    print(res.size)\n","    return res\n","\n","\n","model_wrapper(imgs)\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["from trulens.nn.models import get_model_wrapper\n","\n","wrapped_model = get_model_wrapper(model, input_shape=(1, img_sz, img_sz), device=device)\n","\n","from trulens.nn.attribution import InputAttribution\n","from trulens.nn.attribution import IntegratedGradients\n","\n","infl = InputAttribution(wrapped_model)\n","attrs_input = infl.attributions(imgs.unsqueeze(0))\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["###\n","from captum.attr import GuidedGradCam\n","from captum.attr import visualization as viz\n","\n","# ImageClassifier takes a single input tensor of images Nx3x32x32,\n","# and returns an Nx10 tensor of class probabilities.\n","# It contains an attribute conv4, which is an instance of nn.conv2d,\n","# and the output of this layer has dimensions Nx50x8x8.\n","# It is the last convolution layer, which is the recommended\n","# use case for GuidedGradCAM.\n","\n","\n","guided_gc = GuidedGradCam(model, model.backbones[-1][9])\n","\n","\n","# Computes guided GradCAM attributions for class.\n","# attribution size matches input size, Nx3x32x32\n","attribution = guided_gc.attribute(imgs, 0)\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\n","_ = viz.visualize_image_attr(\n","    np.transpose(attribution.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n","    np.transpose(input.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n","    method=\"blended_heat_map\",\n","    alpha_overlay=0.6,\n",")\n"],"outputs":[],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}